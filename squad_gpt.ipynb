{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d21d0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3eed3e3050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8241753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c3dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('agg_data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2589b809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17250, 612, 0, 220, 2011, 1438, 318, 5180, 986]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"Hi there!  My name is Chris...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea52448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horse', ' fly']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[enc.decode([p]) for p in enc.encode('horse fly')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d94755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab6b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dba562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = sorted(list(set(enc.encode(text))))\n",
    "vocab_size = len(vocabs)\n",
    "encoding = {j: i for i,j in zip(range(vocab_size), vocabs)}\n",
    "decoding = {i: j for i,j in zip(range(vocab_size), vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10411084",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda x: [encoding[i] for i in enc.encode(x)]\n",
    "decode = lambda x: enc.decode([decoding[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6921edfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' maruyama'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(' maruyama'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a018d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d93568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " ' Humph',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " ' melody',\n",
       " '~',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " ' Immortal',\n",
       " 'ÔøΩ',\n",
       " ' Driving',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " ' daytime',\n",
       " '<',\n",
       " ' Drink',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " ' caramel',\n",
       " ' YEAR',\n",
       " ' apologies',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'mite',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " ' MV',\n",
       " ' Geo',\n",
       " 'bugs',\n",
       " '\\n',\n",
       " ' tandem',\n",
       " 'Tank',\n",
       " ' prioritize',\n",
       " ' Mayo',\n",
       " ' ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " 'ÔøΩ',\n",
       " ' t',\n",
       " ' a',\n",
       " 'he',\n",
       " 'in',\n",
       " 're',\n",
       " 'on',\n",
       " ' the',\n",
       " 'er',\n",
       " ' s',\n",
       " 'at',\n",
       " ' w',\n",
       " ' o',\n",
       " 'en',\n",
       " ' c',\n",
       " 'it',\n",
       " 'is',\n",
       " 'an',\n",
       " 'or',\n",
       " 'es',\n",
       " ' b',\n",
       " 'ed',\n",
       " ' f',\n",
       " 'ing',\n",
       " ' p',\n",
       " 'ou',\n",
       " ' an',\n",
       " 'al',\n",
       " 'ar',\n",
       " ' to',\n",
       " ' m',\n",
       " ' of',\n",
       " ' in',\n",
       " ' d',\n",
       " ' h',\n",
       " ' and',\n",
       " 'ic',\n",
       " 'as',\n",
       " 'le',\n",
       " ' th',\n",
       " ' dungeons',\n",
       " 'om',\n",
       " 'ion',\n",
       " 'ent',\n",
       " ' n',\n",
       " ' l',\n",
       " 'st',\n",
       " ' re',\n",
       " 've',\n",
       " 'll',\n",
       " 'ro',\n",
       " 'ly',\n",
       " ' be',\n",
       " ' g',\n",
       " ' T',\n",
       " ' e',\n",
       " ' S',\n",
       " 'id',\n",
       " 'ot',\n",
       " ' I',\n",
       " 'ut',\n",
       " 'et',\n",
       " ' A',\n",
       " ' is',\n",
       " ' on',\n",
       " 'im',\n",
       " 'am',\n",
       " 'ow',\n",
       " 'ay',\n",
       " 'ad',\n",
       " 'se',\n",
       " ' that',\n",
       " ' C',\n",
       " 'ig',\n",
       " ' for',\n",
       " 'ac',\n",
       " ' y',\n",
       " 'ver',\n",
       " 'ur',\n",
       " ' u',\n",
       " 'amazon',\n",
       " ' st',\n",
       " ' M',\n",
       " \"'s\",\n",
       " ' he',\n",
       " ' it',\n",
       " 'eez',\n",
       " 'ith',\n",
       " 'ir',\n",
       " 'ce',\n",
       " ' you',\n",
       " 'il',\n",
       " ' B',\n",
       " ' wh',\n",
       " 'ol',\n",
       " ' P',\n",
       " ' with',\n",
       " ' 1',\n",
       " 'ter',\n",
       " 'ch',\n",
       " ' as',\n",
       " ' we',\n",
       " ' (',\n",
       " 'nd',\n",
       " 'ill',\n",
       " ' Abbey',\n",
       " ' D',\n",
       " ' 2',\n",
       " 'ag',\n",
       " 'ers',\n",
       " 'ke',\n",
       " ' \"',\n",
       " ' H',\n",
       " ' gimm',\n",
       " 'if',\n",
       " ' W',\n",
       " ' R',\n",
       " 'her',\n",
       " ' was',\n",
       " ' r',\n",
       " 'od',\n",
       " ' F',\n",
       " 'ul',\n",
       " 'ate',\n",
       " ' at',\n",
       " 'pp',\n",
       " 'ore',\n",
       " ' The',\n",
       " ' se',\n",
       " 'us',\n",
       " ' pro',\n",
       " ' ha',\n",
       " 'um',\n",
       " ' are',\n",
       " ' de',\n",
       " 'Kh',\n",
       " ' 2022',\n",
       " ' or',\n",
       " 'and',\n",
       " 'est',\n",
       " 'ist',\n",
       " 'ab',\n",
       " ' chi',\n",
       " ' N',\n",
       " 'th',\n",
       " ' com',\n",
       " ' G',\n",
       " 'un',\n",
       " 'op',\n",
       " '00',\n",
       " ' L',\n",
       " ' not',\n",
       " 'ess',\n",
       " ' ex',\n",
       " ' playlist',\n",
       " ' v',\n",
       " ' E',\n",
       " 'res',\n",
       " 'ity',\n",
       " 'ant',\n",
       " ' by',\n",
       " 'el',\n",
       " 'os',\n",
       " 'ew',\n",
       " 'oc',\n",
       " 'qu',\n",
       " ' from',\n",
       " ' have',\n",
       " ' su',\n",
       " 'ort',\n",
       " ' sh',\n",
       " ' this',\n",
       " 'nt',\n",
       " 'ra',\n",
       " 'pe',\n",
       " 'ight',\n",
       " 'art',\n",
       " 'ment',\n",
       " ' al',\n",
       " 'ust',\n",
       " 'end',\n",
       " '--',\n",
       " 'all',\n",
       " ' O',\n",
       " 'ack',\n",
       " ' ch',\n",
       " ' le',\n",
       " 'ies',\n",
       " 'red',\n",
       " 'ard',\n",
       " 'ÔøΩ',\n",
       " 'out',\n",
       " ' J',\n",
       " ' ab',\n",
       " 'ear',\n",
       " 'iv',\n",
       " 'ally',\n",
       " 'our',\n",
       " 'computer',\n",
       " 'gh',\n",
       " 'pt',\n",
       " ' pl',\n",
       " 'ast',\n",
       " ' can',\n",
       " 'ak',\n",
       " 'ome',\n",
       " 'ud',\n",
       " 'The',\n",
       " ' his',\n",
       " ' do',\n",
       " ' go',\n",
       " ' has',\n",
       " 'ge',\n",
       " \"'t\",\n",
       " ' U',\n",
       " ' Assuming',\n",
       " ' sa',\n",
       " ' j',\n",
       " ' but',\n",
       " ' wor',\n",
       " ' all',\n",
       " 'ect',\n",
       " ' k',\n",
       " 'ame',\n",
       " ' will',\n",
       " 'ok',\n",
       " ' they',\n",
       " 'ide',\n",
       " '01',\n",
       " 'ff',\n",
       " 'ich',\n",
       " 'pl',\n",
       " 'ther',\n",
       " ' Weeks',\n",
       " ' tr',\n",
       " '..',\n",
       " 'ie',\n",
       " 'ure',\n",
       " 'age',\n",
       " ' ne',\n",
       " 'ial',\n",
       " 'ap',\n",
       " 'ine',\n",
       " 'ice',\n",
       " ' me',\n",
       " ' out',\n",
       " 'ans',\n",
       " 'one',\n",
       " 'ong',\n",
       " 'ions',\n",
       " ' who',\n",
       " ' K',\n",
       " ' up',\n",
       " ' their',\n",
       " ' ad',\n",
       " ' 3',\n",
       " ' us',\n",
       " 'ated',\n",
       " 'ous',\n",
       " ' more',\n",
       " 'ue',\n",
       " 'og',\n",
       " ' St',\n",
       " 'ind',\n",
       " 'ike',\n",
       " ' so',\n",
       " 'ime',\n",
       " 'per',\n",
       " 'ber',\n",
       " 'iz',\n",
       " 'act',\n",
       " ' one',\n",
       " ' said',\n",
       " ' -',\n",
       " 'are',\n",
       " ' your',\n",
       " 'cc',\n",
       " ' Th',\n",
       " ' cl',\n",
       " 'ep',\n",
       " ' roommate',\n",
       " 'able',\n",
       " 'ake',\n",
       " 'ip',\n",
       " ' which',\n",
       " 'ia',\n",
       " ' im',\n",
       " ' about',\n",
       " ' were',\n",
       " 'very',\n",
       " 'ub',\n",
       " ' had',\n",
       " ' en',\n",
       " ' comp',\n",
       " ' In',\n",
       " ' un',\n",
       " ' ag',\n",
       " 'ire',\n",
       " 'ace',\n",
       " 'au',\n",
       " 'ary',\n",
       " ' would',\n",
       " 'ass',\n",
       " 'ry',\n",
       " ' ÔøΩ',\n",
       " 'cl',\n",
       " 'ook',\n",
       " 'ere',\n",
       " 'so',\n",
       " ' V',\n",
       " 'ign',\n",
       " 'ib',\n",
       " ' off',\n",
       " ' te',\n",
       " 'ven',\n",
       " ' Y',\n",
       " 'ILY',\n",
       " 'ose',\n",
       " 'ite',\n",
       " ' mattered',\n",
       " ' 201',\n",
       " ' res',\n",
       " ' man',\n",
       " ' per',\n",
       " ' other',\n",
       " 'ord',\n",
       " 'ult',\n",
       " ' been',\n",
       " ' like',\n",
       " 'ase',\n",
       " 'ance',\n",
       " 'ks',\n",
       " ' demolished',\n",
       " 'ays',\n",
       " 'ence',\n",
       " 'own',\n",
       " 'ction',\n",
       " ' any',\n",
       " ' app',\n",
       " ' sp',\n",
       " 'int',\n",
       " ' dis',\n",
       " 'ations',\n",
       " ' blah',\n",
       " ' 4',\n",
       " 'ical',\n",
       " ' them',\n",
       " ' her',\n",
       " 'ount',\n",
       " ' Ch',\n",
       " ' if',\n",
       " ' there',\n",
       " ' pe',\n",
       " ' year',\n",
       " 'av',\n",
       " ' my',\n",
       " ' some',\n",
       " ' when',\n",
       " 'ough',\n",
       " 'ach',\n",
       " ' than',\n",
       " 'ru',\n",
       " 'ond',\n",
       " 'ick',\n",
       " ' over',\n",
       " ' mattress',\n",
       " ' qu',\n",
       " '\\n\\n',\n",
       " ' sc',\n",
       " 'ree',\n",
       " ' It',\n",
       " 'ound',\n",
       " 'port',\n",
       " ' also',\n",
       " ' part',\n",
       " ' kn',\n",
       " 'testing',\n",
       " ' time',\n",
       " 'ens',\n",
       " ' 5',\n",
       " ' what',\n",
       " ' no',\n",
       " 'du',\n",
       " 'mer',\n",
       " 'ang',\n",
       " ' new',\n",
       " ' get',\n",
       " 'ory',\n",
       " 'ings',\n",
       " ' just',\n",
       " ' into',\n",
       " ' 0',\n",
       " 'ents',\n",
       " 'te',\n",
       " ' people',\n",
       " ' pre',\n",
       " ' its',\n",
       " ' rec',\n",
       " ' tw',\n",
       " 'ian',\n",
       " 'ark',\n",
       " 'ors',\n",
       " ' work',\n",
       " 'ade',\n",
       " 'ob',\n",
       " ' she',\n",
       " ' our',\n",
       " ' Worst',\n",
       " 'ink',\n",
       " ' 19',\n",
       " ' He',\n",
       " 'ish',\n",
       " ' 2021',\n",
       " ' him',\n",
       " 'ons',\n",
       " ' drawer',\n",
       " ' ro',\n",
       " 'form',\n",
       " ' [',\n",
       " 'ates',\n",
       " 'vers',\n",
       " ' only',\n",
       " 'oll',\n",
       " ' Allison',\n",
       " 'ild',\n",
       " 'ell',\n",
       " 'amp',\n",
       " ' conco',\n",
       " ' bl',\n",
       " ' Mines',\n",
       " 'urn',\n",
       " 'ft',\n",
       " 'ood',\n",
       " ' how',\n",
       " ' enthusiast',\n",
       " \" '\",\n",
       " ' after',\n",
       " 'hed',\n",
       " ' att',\n",
       " 'ov',\n",
       " ' ABOUT',\n",
       " ' play',\n",
       " 'ne',\n",
       " 'erv',\n",
       " ' could',\n",
       " 'itt',\n",
       " ' am',\n",
       " ' first',\n",
       " ' 6',\n",
       " ' act',\n",
       " ' $',\n",
       " 'ec',\n",
       " 'hing',\n",
       " 'Wa',\n",
       " 'ual',\n",
       " 'oy',\n",
       " 'old',\n",
       " ' fe',\n",
       " ' bet',\n",
       " 'we',\n",
       " 'iff',\n",
       " ' two',\n",
       " 'ock',\n",
       " ' back',\n",
       " ').',\n",
       " 'ident',\n",
       " ' under',\n",
       " 'sel',\n",
       " 'xt',\n",
       " ' may',\n",
       " ' Birthday',\n",
       " ' po',\n",
       " 'ph',\n",
       " 'iss',\n",
       " ' des',\n",
       " ' most',\n",
       " ' did',\n",
       " ' add',\n",
       " 'ject',\n",
       " ' driveway',\n",
       " 'ont',\n",
       " ' again',\n",
       " ' know',\n",
       " ' need',\n",
       " ' co',\n",
       " ' .',\n",
       " ' want',\n",
       " ' see',\n",
       " ' 7',\n",
       " ' This',\n",
       " ' even',\n",
       " ' ind',\n",
       " 'ty',\n",
       " ' We',\n",
       " 'ath',\n",
       " ' these',\n",
       " ' pr',\n",
       " ' use',\n",
       " ' because',\n",
       " ' fl',\n",
       " 'ng',\n",
       " ' now',\n",
       " ' exclaimed',\n",
       " 'com',\n",
       " 'ise',\n",
       " ' make',\n",
       " ' then',\n",
       " 'ower',\n",
       " ' every',\n",
       " ' Un',\n",
       " ' sec',\n",
       " 'oss',\n",
       " ' Definitely',\n",
       " ' em',\n",
       " ' =',\n",
       " ' Re',\n",
       " 'stri',\n",
       " 'rit',\n",
       " ' mandates',\n",
       " ' bowling',\n",
       " 'ied',\n",
       " 'ating',\n",
       " ' look',\n",
       " 'man',\n",
       " 'pect',\n",
       " ' 8',\n",
       " 'row',\n",
       " ' bu',\n",
       " ' where',\n",
       " 'ific',\n",
       " ' years',\n",
       " 'ily',\n",
       " ' should',\n",
       " ' rem',\n",
       " 'Th',\n",
       " 'ATS',\n",
       " 'In',\n",
       " 'day',\n",
       " \"'re\",\n",
       " ' rel',\n",
       " 'ss',\n",
       " ' def',\n",
       " ' right',\n",
       " '),',\n",
       " 'les',\n",
       " '000',\n",
       " 'hen',\n",
       " ' through',\n",
       " ' Tr',\n",
       " 'uttering',\n",
       " ' way',\n",
       " ' don',\n",
       " ' ,',\n",
       " ' 10',\n",
       " 'ased',\n",
       " ' ass',\n",
       " ' And',\n",
       " 'ix',\n",
       " ' very',\n",
       " 'other',\n",
       " ' imp',\n",
       " 'oth',\n",
       " ' sub',\n",
       " ' ‚Äî',\n",
       " ' being',\n",
       " 'arg',\n",
       " ' Wh',\n",
       " ' does',\n",
       " ' mastered',\n",
       " ' 9',\n",
       " 'ert',\n",
       " 'ps',\n",
       " 'ited',\n",
       " ' Alison',\n",
       " ' br',\n",
       " ' down',\n",
       " ' many',\n",
       " 'aking',\n",
       " ' call',\n",
       " '314',\n",
       " 'ities',\n",
       " ' ph',\n",
       " ' bullied',\n",
       " ' FIN',\n",
       " 'als',\n",
       " 'ative',\n",
       " ' dec',\n",
       " ' before',\n",
       " 'ics',\n",
       " ' well',\n",
       " ' much',\n",
       " 'ener',\n",
       " ' those',\n",
       " ' such',\n",
       " ' ke',\n",
       " ' end',\n",
       " ' But',\n",
       " ' honorable',\n",
       " 'ting',\n",
       " ' long',\n",
       " 'ef',\n",
       " ' think',\n",
       " 'ys',\n",
       " ' bel',\n",
       " ' sm',\n",
       " 'its',\n",
       " 'ax',\n",
       " ' own',\n",
       " ' set',\n",
       " 'ments',\n",
       " 'ble',\n",
       " ' easing',\n",
       " ' show',\n",
       " 'ms',\n",
       " ' say',\n",
       " ' Sh',\n",
       " 'ts',\n",
       " 'ful',\n",
       " ' gu',\n",
       " ' inst',\n",
       " 'und',\n",
       " 'ren',\n",
       " ' Takes',\n",
       " ' ent',\n",
       " ' You',\n",
       " ' good',\n",
       " ' start',\n",
       " ' made',\n",
       " ' mustard',\n",
       " 'tt',\n",
       " ' copyrighted',\n",
       " 'up',\n",
       " 'olog',\n",
       " 'ump',\n",
       " ' |',\n",
       " 'ular',\n",
       " 'ually',\n",
       " ' ac',\n",
       " ' last',\n",
       " ' 200',\n",
       " '10',\n",
       " ' stud',\n",
       " 'self',\n",
       " 'ars',\n",
       " 'cy',\n",
       " ' min',\n",
       " 'io',\n",
       " ' mod',\n",
       " ' count',\n",
       " ' Com',\n",
       " ' intrigued',\n",
       " ' fin',\n",
       " 'air',\n",
       " 'ier',\n",
       " '‚Äî',\n",
       " 'read',\n",
       " 'ank',\n",
       " 'hes',\n",
       " ' Advent',\n",
       " ' Positive',\n",
       " ' point',\n",
       " 'ork',\n",
       " ' New',\n",
       " 'ool',\n",
       " ' used',\n",
       " 'ween',\n",
       " ' same',\n",
       " 'oun',\n",
       " ' Al',\n",
       " 'ci',\n",
       " ' while',\n",
       " ' game',\n",
       " ' sim',\n",
       " '...',\n",
       " 'ek',\n",
       " ' report',\n",
       " ' still',\n",
       " 'led',\n",
       " 'ah',\n",
       " ' here',\n",
       " ' world',\n",
       " ' though',\n",
       " ' Crusader',\n",
       " 'ale',\n",
       " ' Se',\n",
       " ' If',\n",
       " ' Le',\n",
       " ' ret',\n",
       " ' trans',\n",
       " 'ner',\n",
       " ' take',\n",
       " ' Cl',\n",
       " ' conf',\n",
       " 'way',\n",
       " 'ave',\n",
       " ' going',\n",
       " ' sl',\n",
       " 'ug',\n",
       " ' astronaut',\n",
       " ' hand',\n",
       " ' between',\n",
       " 'ists',\n",
       " ' De',\n",
       " 'oot',\n",
       " 'It',\n",
       " ' ear',\n",
       " ' against',\n",
       " ' high',\n",
       " 'gan',\n",
       " 'az',\n",
       " ' uploading',\n",
       " ' exp',\n",
       " ' ins',\n",
       " ' gr',\n",
       " ' help',\n",
       " 'ets',\n",
       " 'ins',\n",
       " ' Pro',\n",
       " 'ism',\n",
       " ' found',\n",
       " 'land',\n",
       " 'uss',\n",
       " 'ames',\n",
       " ' person',\n",
       " ' great',\n",
       " 'pr',\n",
       " ' sign',\n",
       " ' An',\n",
       " \"'ve\",\n",
       " ' ser',\n",
       " ' run',\n",
       " ' :',\n",
       " ' christ',\n",
       " ' follow',\n",
       " 'ices',\n",
       " ' find',\n",
       " '12',\n",
       " ' mem',\n",
       " ' cr',\n",
       " 'ered',\n",
       " ' Incredible',\n",
       " 'ex',\n",
       " 'uth',\n",
       " ' accommodations',\n",
       " 'co',\n",
       " ' team',\n",
       " 'ash',\n",
       " 'att',\n",
       " 'ved',\n",
       " ' system',\n",
       " ' As',\n",
       " 'der',\n",
       " ' extras',\n",
       " ' bye',\n",
       " ' lead',\n",
       " 'min',\n",
       " 'ives',\n",
       " ' around',\n",
       " ' enlist',\n",
       " ' cur',\n",
       " ' cour',\n",
       " 'ages',\n",
       " 'ize',\n",
       " ' car',\n",
       " 'ode',\n",
       " 'IQ',\n",
       " ' read',\n",
       " \"'m\",\n",
       " 'con',\n",
       " ' real',\n",
       " ' support',\n",
       " ' 12',\n",
       " ' law',\n",
       " ' really',\n",
       " 'ness',\n",
       " ' fact',\n",
       " ' day',\n",
       " ' both',\n",
       " 'ying',\n",
       " ' rooftop',\n",
       " ' For',\n",
       " ' three',\n",
       " 'Gs',\n",
       " ' They',\n",
       " '50',\n",
       " ' each',\n",
       " 'akes',\n",
       " ' che',\n",
       " ' cre',\n",
       " ' exiting',\n",
       " 'ines',\n",
       " '19',\n",
       " 'gg',\n",
       " 'ute',\n",
       " 'ik',\n",
       " 'We',\n",
       " 'get',\n",
       " 'ER',\n",
       " ' met',\n",
       " ' says',\n",
       " ' endlessly',\n",
       " ' during',\n",
       " 'ox',\n",
       " 'ized',\n",
       " 'ared',\n",
       " ' fam',\n",
       " 'ically',\n",
       " ' Is',\n",
       " 'med',\n",
       " 'vent',\n",
       " 'ient',\n",
       " 'iet',\n",
       " 'rent',\n",
       " '11',\n",
       " ' 20',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[enc.decode([i]) for i in list(set(enc.encode(text)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13c212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e625ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093e0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0b13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54bb4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d891a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89aff082",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd4cf3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.42272 M parameters\n"
     ]
    }
   ],
   "source": [
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbbcb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33a4a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.0381, val loss 3.5478\n"
     ]
    }
   ],
   "source": [
    "for iter in range(100):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48050653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(2.5027), 'val': tensor(3.3205)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72f14138",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "ans = m.generate(context, max_new_tokens=500)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a19d7632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!rentices36 I will take name%)\n",
      "conserv has your Cain thing! why!\n",
      "\n",
      "Samvrutha Tumuluru reacted with ‚ù§ today\n",
      "Samvrutha Tumuluru: I each of your question\n",
      "SamvruthaÔøΩicut made all your dad at the 24 reacted with üëç\n",
      "Samvrutha Tumuluru: :(\n",
      "Samvrutha just curious\n",
      "Yuta Baba Tumuluru: Thank you made a photo.\n",
      "Samvrutha Tumuluru: How‚Äôs go lol\n",
      "Daniel Strizhevsky: Owhose on Luigi 4 Drink again\n",
      "Daniel Strizhevsky: Holy cow\n",
      "Samvrutha Tumuluru: Good moment\n",
      "Zach Gospe: Flying out\n",
      "Daniel Strizhevsky: And let it if we have not Bits?\n",
      "Samvrutha Tumuluru reacted with üòÆ\n",
      "Chris Acker: Are dozens last while\n",
      "Chris Acker: Welcome to move the \"AvÔøΩ\n",
      "Samvrutha Tumuluru: Ieden\n",
      "Samvrutha Tumuluru: strizhevsky:benefit after the SnTA welcome to do so we will camp is free rice\n",
      "Chris Acker: Lol track due to the desktop ticket longer rooms\n",
      "Samvrutha Tumuluru: @Fth\n",
      "Daniel Strizhevsj fearful270\n",
      "Daniel Strizhevsky:Hi after mymorolling\n",
      "Daniel Strizhevsky: I means I doubt they're interested\n",
      "Daniel Strizhevsky: We are u sir\n",
      "Daniel Strizhevsky: Holy cow or taking no longer 269 4 hrsmo*\n",
      "Samvruth million427 made a couple laundry\n",
      "Chris Acker started a video roll\n",
      "Samvrutha Tumuluru: What is?\n",
      "Zach Gospe: You bought vote boy\n",
      "Samvrutha Tumuluru reacted with üòÜ\n",
      "Chris to make open the coffee borutha Tumuluru reacted with from me miso sure I darn\n",
      "Samvrutha Tumuluru: I're yak\n",
      "Zach Gospe: https:// Kathryn Puppytaba set\n",
      "Zach Gospe: oh restaurants!\n",
      "Samvrutha Tumuluru: so we alsobuckB Bengal.\n",
      "Zach Gospe: Thx\n",
      "Zach Gospe: No I!!!\n",
      " scapego\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6ac72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
